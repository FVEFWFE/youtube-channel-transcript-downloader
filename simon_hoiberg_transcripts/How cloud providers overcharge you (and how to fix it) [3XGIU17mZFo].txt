Title: How cloud providers overcharge you (and how to fix it)
Video ID: 3XGIU17mZFo
URL: https://www.youtube.com/watch?v=3XGIU17mZFo
================================================================================

I'm convinced that cloud providers are designed exactly like casinos. The architecture, the UI, and specifically the pricing models. They're all engineered to make you lose track of how much you're actually spending. It's never one big purchase. It's a fraction of a cent here, a tiny data transfer fee there, and before you know it, you are bleeding thousands of dollars a year on resources you don't even use. I spent the last week auditing my own infrastructure and I found six specific traps. These are sneaky default settings and misleading pricing structures that providers like AWS and Versel use to inflate your bill without you noticing. Today, I'm going to show you exactly what they are and how to fix them. The first trap happens on AWS. I call this the Cloudatch buildup trap. Here's what happens. You spin up a new resource. It could be a Lambda function, an RDS instance, or maybe a new ECS cluster. Automatically, AWS creates a Cloudatch lock group associated with it. Now, here's what many people don't realize. The default retention policy for that new lock group is always set to never expire. And AWS is billing you for logs in two ways. You pay to ingest the data, but you also pay a monthly fee for every gigabyte stored. And because the default retention policy is set to never expire, that storage cost accumulates forever. You're literally paying monthly rent for gigabytes of old text logs from Lambda calls that happened years ago. And over time, this actually becomes pretty significant. So if you like using Cloudatch to search through your logs, the easiest way to fix this is to simply change the retention period to something like 5 or 7 days, whatever works for you. But if you're anything like me and you think Cloudatch is a terrible user and developer experience, there's an even sneaker and much more aggressive way to fight this. Set up Grafana on a cheap self-hosted server or VPS. Create a Cloudatch data source in Grafana and configure this to pull your log groups at an interval, for instance, every 30 seconds. This will move all of your logs to your own server which you can host for a fraction of the cost and make all the logs available through Graphana which in my opinion is a much better experience. Now go back to Cloudatch on AWS and set the retention to one day. There you go. You made your life more pleasant and you're saving a bunch of money at the same time. And if you want to double down, set up a cron job in GitHub actions to occasionally run through all regions, find all log groups, and update the retention period on all of them, just in case you created a new resource and forgot to do this manually. The second trap is what I call the bandwidth trap. We all want our landing pages to look slick. So, you decide to add a highquality looping background video in the hero section or a promo video to showcase your tool. The file is only 10 megabytes, so you committed to a repo and push to production. No big deal, right? Well, this is where data egress fees come in to kill your margins. Let's do the math. If just 1,000 people visit your site, that little 10 megabyte file turns into 10 GB of data transfer. If you go viral and get a 100,000 visitors, that is a terabyte of data. Now, if you're hosting on a platform like Versel, their pro plan gives you 1 terbte included. But the moment you cross that line, they charge you $0.15 for every extra gigabyte. That bill scales up pretty fast and it's just a terrible way to burn cash. The quick fix here is to upload that video to YouTube or Vimeo and just embed their player. I'm doing that on one of my websites. It works pretty well and you're essentially letting Google pay your bandwidth bill. Though, if you want that clean native HTML video element, here's a better way to do it. host your heavy assets on Cloudflare R2. Unlike S3 on AWS or other big providers, Cloudflare R2 charges you exactly $0 for egress bandwidth. You only pay a tiny amount for the storage itself. So in this way, you can get the performance of a global CDN, the clean look of a native video tag, and you completely bypass the bandwidth trap, which leads me to the third trap, what I call the optimization trap. If you're using Nex.js, XJS, you probably use their magic image component. You know, the one that automatically resizes and compresses photos for every device size. But here's the thing. Platforms like Bracel charge you for those image optimizations. And their limits are actually pretty low, something like a,000 transformations. If you run a directory site or website with a lot of images, you will blow through that quota in days and suddenly you are paying premium prices just to show pictures. Now, I know the standard advice is to configure a custom loader or tweak the settings, but honestly, my fix is simple. Stop using the image component and just use the standard HTML image tag. Instead of relying on the server to resize images on the fly, simply save your images as WEBP files at a reasonable oneizefits-all resolution. This serves a high quality image that looks great on both desktop and mobile, and it doesn't touch Versel's processing limits at all. Now, I know lighthouse score, web vitals, blah blah. The obsession with microoptimizing load speeds has been completely blown out of proportion. We're now living in the world of 5G and fiber internet. The difference in load time between a perfectly optimized responsive image and a standard WEBP file is probably unnoticeable to your users. And Google's penalty for unoptimized images is completely blown out of proportion to. I'm firmly convinced that highquality content outweighs a fraction of a second in load time. So write a simple script that uses Shark to batch convert your entire media folder to WEBP during build time. You get 90% of the optimization benefits, but with 0% of the recurring cost or vendor lock. The fourth trap is what I call the region premium trap. This isn't a super common one, but it does happen. And most cloud providers like AWS, you pick a region. And naturally, most people just pick the city that is physically closest to where they live. If you're in the UK, you pick London. If you're in Brazil, you pick S. Paulo. If you're in South Africa, you pick Cape Town. But here's the mistake. Cloud pricing is not global. It's local. Because of higher taxes, electricity cost, and infrastructure challenges, servers in places like S. Paulo or Cape Town can be up to 50% more expensive than the exact same servers in the US. So unless you have strict legal requirements that force you to keep data within your country's borders or you're building a tool where every millisecond counts, just pick US- East. I literally pick US- East one for almost all of my products. This is almost always the cheapest region available. For 99% of SAS applications, the latency difference is negligible, but the savings on your monthly bill can be quite significant. If you're in the EU and you want your data processing agreement to say that your data centers are in the EU, you may want to pick one of the EU regions, though. But as far as I know, AWS actually can't guarantee that data strictly stays in that region anyway. So, not sure if it solves anything really. The next trap is what I call the NoSQL hype trap. There's this massive narrative in the tech world that you need to be serverless and use databases like DynamoB. And honestly, I thought so too for a long long time. I thought it was super convenient that I didn't have to worry about how my databases were hosted. They were just there, always available and with backups running automatically. But the downside is Dynamob and other serverless databases charge you per operation. So every time you read, every time you write, every time you query or scan, you're charged. And this is only financially viable if your data structure is perfectly optimized for specific access patterns. And you often have to use complex strategies like single table design to make that math work. And the reality for us founders is that we don't know what's going to happen tomorrow. We pivot. We add new features. We change how we want to display data. At the moment your query needs change, your perfectly optimized NoSQL database kind of loses its purpose. You end up running inefficient scans and queries and it just end up costing a lot of money. At some point, you may even feel inclined to avoid certain features or to sacrifice good user experience just to avoid making too many reads or writes. The fix is simple. Stick to boring technology. Use Postgress or MySQL. SQL is much more forgiving. You can pivot your products, add new features, and query your data in ways you didn't plan for without having to worry about how it affect costs. And best of all, you pay a flat monthly fee for the server box. It doesn't matter if your queries are messy or complex. The price is the same. The final trap is what I call the zombie resource trap. This one is extremely common because of how cloud providers decouple their services. For instance, they treat the computer, the hard drive, and the load balancer as three completely separate products. So, here's the trap. You decide to shut down your project to save money. You go into the console and delete the server instance, and you think you're done, but you're not. Often, the load balancer does not get deleted with the server. It stays behind because it's no longer attached to anything. doesn't show up on your main dashboard, but you're still paying about $18 a month for a zombie load balancer that is routing traffic to absolutely nowhere. To fix this, you need to stop clicking around through random tabs to find things. Go to the search bar and type tag editor. It is inside the AWS resource groups. In the settings, set regions to all regions. Set resource types to all supported resource types and hit search. This gives you a full list of every single item currently alive in your account. Scroll through that list. If you see a load balancer or hard drive volume or any other resource for a project you thought you deleted, kill it immediately. If you're setting up a whole bunch of resources, I can highly recommend using an infrastructure as code tool like AWS CDK. This makes it much easier to keep things organized, and it saves you from a lot of clicking around the UI, and it's much easier to clean up all the resources and dependencies using a single command. If you've been watching my videos for a while, you probably noticed that I changed things up a bit here in my last few videos. I simplified my studio and relaxed the editing a bit, but I also changed the software I'm using. I used to do all of my videos using Adobe Premiere and Adobe After Effects, but I recently changed to Da Vinci Resolve. And honestly, I'm blown away by the quality of this software that I always saw as a bit of an underdog. So, instead of paying an expensive monthly fee to Adobe, which, let's face it, are known for using some pretty aggressive and predatory practices, I made a one-time purchase and got this new software, Da Vinci Resolve, which is arguably a much better tool. And I'm honestly kind of embarrassed that it took me so long. And it got me thinking generally, not just specific to cloud providers or editing tools, I think we should try looking for the underdog a bit more often cuz I really think it's insane how much value we can get for much less just by not defaulting to these huge established companies. And in all honesty, it's how I try to position myself as well with my tools. That's also why on my main website, I put my tools right next to these big established tools. So, it's super clear what my tools are aiming to replace and how much more affordable they are. And I'm biased, of course, but I honestly believe that the tools my team and I built are simply much higher quality products. And we do have a lot of testimonials confirming that, too. This isn't a political channel, so I'm not going to get deep into this, but I personally think that we got a bit lazy and that we're slowly allowing a small handful of mega corporations to basically build monopoly. And one small thing we could do is instead of just defaulting to these three big companies we already heard of when we have a new problem, maybe spend just a little time scratching a little deeper and see if you can find a lesserk known alternative. In this way, we can spread out where revenue goes a bit more and in most cases actually end up with a much better solution to whatever problem we had. So, just a small encouragement from here to try to support the underdog a bit more often. So, here's my challenge to you. Don't just click away from this video. Open your AWS console or your Versell dashboard right now. Audit those logs. Check those regions. Kill those zombie resources. And if you go through this list and realize that the cloud has become too much of a headache, well, maybe it's time to pack your bags. I recently published a full breakdown of why I left the cloud entirely and moved to self-hosted servers. You can watch that one right here.